{"cells":[{"cell_type":"markdown","source":["### Imports :"],"metadata":{"id":"Tsdff5bu1_GR"}},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-06-02T13:52:03.896453Z","iopub.status.busy":"2021-06-02T13:52:03.896085Z","iopub.status.idle":"2021-06-02T13:52:47.3357Z","shell.execute_reply":"2021-06-02T13:52:47.334231Z","shell.execute_reply.started":"2021-06-02T13:52:03.896422Z"},"id":"pF75m3K11JgF","executionInfo":{"status":"ok","timestamp":1730996925150,"user_tz":-60,"elapsed":3102,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"outputs":[],"source":["import pandas as pd\n","\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","from torch.nn import TransformerDecoder, TransformerDecoderLayer\n","import torch.nn.functional as F\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import numpy as np\n","\n","import math\n","import random\n","\n","import os\n","import re\n","from tqdm import tqdm\n","\n","from transformers import AutoModel\n","from transformers import AutoTokenizer\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset, Sampler\n","\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold\n","\n","import time"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pJKGzNnX3ySv","executionInfo":{"status":"ok","timestamp":1730996937349,"user_tz":-60,"elapsed":2364,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}},"outputId":"7545d8e2-d9a4-4ecb-d070-f80fd01b53a4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["cd drive/MyDrive/TOUNSI"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ue1ouAne5GVp","executionInfo":{"status":"ok","timestamp":1730996940416,"user_tz":-60,"elapsed":364,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}},"outputId":"7525042f-0a89-4080-ea05-0376f7378b92"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/TOUNSI\n"]}]},{"cell_type":"code","source":["seed = 99"],"metadata":{"id":"0iESffba3j2t","executionInfo":{"status":"ok","timestamp":1730996942600,"user_tz":-60,"elapsed":7,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Setting the seed :"],"metadata":{"id":"aX-MNSV12DNT"}},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-06-02T13:53:01.192636Z","iopub.status.busy":"2021-06-02T13:53:01.192236Z","iopub.status.idle":"2021-06-02T13:53:01.207015Z","shell.execute_reply":"2021-06-02T13:53:01.206017Z","shell.execute_reply.started":"2021-06-02T13:53:01.192598Z"},"id":"R6vckLDU1JiJ","executionInfo":{"status":"ok","timestamp":1730997190332,"user_tz":-60,"elapsed":356,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"outputs":[],"source":["def set_seed():\n","    \"\"\"Set seed for reproducibility.\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","set_seed()"]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"_WpOoq4M_WuA","executionInfo":{"status":"ok","timestamp":1730997191907,"user_tz":-60,"elapsed":6,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"bok0xiSxI0tD","executionInfo":{"status":"ok","timestamp":1730997192233,"user_tz":-60,"elapsed":22,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}},"outputId":"23aaefda-70d8-4cb2-a2ee-e15a6244e6bd"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["## Importing data :"],"metadata":{"id":"fRE_gDLS2Hr3"}},{"cell_type":"code","source":["data_path = \"Code/data/external/transliteration/dataset.xlsx\""],"metadata":{"id":"P_Qisz1g5pgf","executionInfo":{"status":"ok","timestamp":1730997198287,"user_tz":-60,"elapsed":646,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-06-02T13:53:01.209287Z","iopub.status.busy":"2021-06-02T13:53:01.208855Z","iopub.status.idle":"2021-06-02T13:53:03.987754Z","shell.execute_reply":"2021-06-02T13:53:03.986452Z","shell.execute_reply.started":"2021-06-02T13:53:01.209241Z"},"id":"ypOSFCBz1JiS","executionInfo":{"status":"ok","timestamp":1730997199812,"user_tz":-60,"elapsed":1535,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"outputs":[],"source":["xls = pd.ExcelFile(\"Code/data/external/transliteration/dataset.xlsx\")\n","dataset = pd.read_excel(xls, \"Sheet1\")\n","\n","known = dataset[dataset.from_source == True]\n","\n","dataset = dataset[[\"arabizi\", \"arabic\", \"from_source\"]]\n","dataset.columns = [\"Arabize\", \"Arabic\", \"from_source\"]"]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"MtR1f8Z67XTe","executionInfo":{"status":"ok","timestamp":1730997199813,"user_tz":-60,"elapsed":22,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}},"outputId":"fcd154bb-dcaf-4816-e11b-854b1849ee7d"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             Arabize      Arabic  from_source\n","0                  w           و         True\n","1                 ya          يا         True\n","2                 fi          في         True\n","3               rabi         ربي         True\n","4                 el          ال         True\n","...              ...         ...          ...\n","16849        2ejilan        آجلا        False\n","16850        elhewya     الهاوية        False\n","16851         5altin      خالطين        False\n","16852         nadhra        نظرة        False\n","16853  mayse3idhomch  مايساعدهمش        False\n","\n","[16854 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-2357428a-0caf-4a7c-accd-dd98fd2c461e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Arabize</th>\n","      <th>Arabic</th>\n","      <th>from_source</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>w</td>\n","      <td>و</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ya</td>\n","      <td>يا</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>fi</td>\n","      <td>في</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>rabi</td>\n","      <td>ربي</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>el</td>\n","      <td>ال</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>16849</th>\n","      <td>2ejilan</td>\n","      <td>آجلا</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>16850</th>\n","      <td>elhewya</td>\n","      <td>الهاوية</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>16851</th>\n","      <td>5altin</td>\n","      <td>خالطين</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>16852</th>\n","      <td>nadhra</td>\n","      <td>نظرة</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>16853</th>\n","      <td>mayse3idhomch</td>\n","      <td>مايساعدهمش</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>16854 rows × 3 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2357428a-0caf-4a7c-accd-dd98fd2c461e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-2357428a-0caf-4a7c-accd-dd98fd2c461e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-2357428a-0caf-4a7c-accd-dd98fd2c461e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-e8d096e7-f068-4825-a917-bd86ccb60213\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e8d096e7-f068-4825-a917-bd86ccb60213')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-e8d096e7-f068-4825-a917-bd86ccb60213 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_f5214e77-e811-44f0-8d1a-9f042583b4b8\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dataset')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_f5214e77-e811-44f0-8d1a-9f042583b4b8 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('dataset');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"dataset","summary":"{\n  \"name\": \"dataset\",\n  \"rows\": 16854,\n  \"fields\": [\n    {\n      \"column\": \"Arabize\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14266,\n        \"samples\": [\n          \"lmaghreb\",\n          \"naamel\",\n          \"makboula\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Arabic\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11093,\n        \"samples\": [\n          \"\\u062d\\u0642\\u0646\\u0627\",\n          \"\\u0648\\u0648\\u0648\\u0648\\u0648\\u0648\",\n          \"\\u062d\\u0644\\u062a\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"from_source\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["## Preprocessing, Tokenisation, Spliting :"],"metadata":{"id":"dJDwyiao2LH-"}},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2021-06-02T13:53:03.989821Z","iopub.status.busy":"2021-06-02T13:53:03.989368Z","iopub.status.idle":"2021-06-02T13:53:04.003371Z","shell.execute_reply":"2021-06-02T13:53:04.001919Z","shell.execute_reply.started":"2021-06-02T13:53:03.989771Z"},"id":"qNAYiikm1JiZ","executionInfo":{"status":"ok","timestamp":1730997206588,"user_tz":-60,"elapsed":615,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"outputs":[],"source":["# By using known, the transliteration function can check if a word is already in the dictionary and directly retrieve the Arabic version. This saves the model from recomputing the transliteration,\n","# which is especially useful for common words or frequently repeated text.\n","# It also improves accuracy by avoiding potential errors from the model on words it has previously handled correctly.\n","known = known[[\"arabizi\", \"arabic\"]].set_index(\"arabizi\", drop=True).arabic.to_dict()\n","known_idx = list(known.keys())"]},{"cell_type":"code","source":["# Calculate the maximum length of the Arabizi text in the dataset : This will help set the maximum input sequence length for the model\n","in_max = dataset.apply(lambda x: len(str(x.Arabize)), axis=1).max()\n","\n","# Calculate the maximum length of the \"Arabic\" text in the dataset : The \"+ 2\" accounts for the sos and eos tokens\n","out_max = dataset.apply(lambda x: len(x.Arabic), axis=1).max() + 2\n","\n","# Define token values for padding, end of sequence, and start of sequence\n","pad_token = 0  # Token used to pad sequences to the same length\n","eos_token = 2  # Token indicating the end of a sequence\n","sos_token = 1  # Token indicating the start of a sequence"],"metadata":{"id":"TFf2Mxq5-BHw","executionInfo":{"status":"ok","timestamp":1730997207439,"user_tz":-60,"elapsed":856,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2021-06-02T13:53:04.596889Z","iopub.status.busy":"2021-06-02T13:53:04.596542Z","iopub.status.idle":"2021-06-02T13:53:04.605661Z","shell.execute_reply":"2021-06-02T13:53:04.604277Z","shell.execute_reply.started":"2021-06-02T13:53:04.596855Z"},"id":"gom0R8b91Jif","executionInfo":{"status":"ok","timestamp":1730997208045,"user_tz":-60,"elapsed":9,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"outputs":[],"source":["def preprocess(a):\n","\n","    x = a.copy()\n","\n","    def filter_letters_arabizi(word):\n","\n","        word = word.replace(\"$\", \"s\")\n","        word = word.replace(\"å\", \"a\")\n","        word = word.replace(\"é\", \"e\")\n","        word = word.replace(\"ê\", \"e\")\n","        word = word.replace(\"ÿ\", \"y\")\n","        word = word.replace(\"ą\", \"a\")\n","        word = word.replace(\"ī\", \"i\")\n","        word = word.replace(\"\\n\", \"\")\n","        word = word.replace(\"′\", \"'\")\n","\n","        return word\n","\n","    x.Arabize = filter_letters_arabizi(str(x.Arabize))\n","    x.Arabic = x.Arabic\n","\n","    return x"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2021-06-02T13:53:04.608063Z","iopub.status.busy":"2021-06-02T13:53:04.607601Z","iopub.status.idle":"2021-06-02T13:53:08.040548Z","shell.execute_reply":"2021-06-02T13:53:08.039363Z","shell.execute_reply.started":"2021-06-02T13:53:04.608016Z"},"id":"NCbtTwoJ1Jio","executionInfo":{"status":"ok","timestamp":1730997212504,"user_tz":-60,"elapsed":2821,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"outputs":[],"source":["dataset[[\"Arabize\",\"Arabic\"]] = dataset[[\"Arabize\",\"Arabic\"]].apply(preprocess, axis=1)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2021-06-02T13:53:08.041925Z","iopub.status.busy":"2021-06-02T13:53:08.041652Z","iopub.status.idle":"2021-06-02T13:53:08.059417Z","shell.execute_reply":"2021-06-02T13:53:08.058574Z","shell.execute_reply.started":"2021-06-02T13:53:08.041898Z"},"id":"d1Ba7UOR1Jit","executionInfo":{"status":"ok","timestamp":1730997212506,"user_tz":-60,"elapsed":13,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"outputs":[],"source":["# Create a set of all unique tokens (words) in the Arabizi (input) text, converted to lowercase\n","in_tokens = set(\" \".join(dataset.Arabize.values.tolist()).lower())\n","\n","# Create a dictionary mapping each unique token to an integer, starting from 1 (0 is reserved for padding)\n","in_token_to_int = {token: (i+1) for i, token in enumerate(sorted(in_tokens))}\n","\n","# Add a pad token mapping to 0, as it's often reserved for padding\n","in_token_to_int[0] = \"<pad>\""]},{"cell_type":"code","source":["# Create a set of all unique tokens (words) in the Arabic (output) text\n","out_tokens = set(\" \".join(dataset.Arabic.values.tolist()))\n","\n","# Create a dictionary mapping each unique token to an integer, starting from 3 (0, 1, and 2 are reserved for pad, sos, and eos)\n","out_token_to_int = {token: (i+3) for i, token in enumerate(sorted(out_tokens))}\n","\n","# Add special tokens for padding, start of sequence (sos), and end of sequence (eos) to the Arabic token dictionary\n","out_token_to_int[\"<pad>\"] = pad_token  # Padding token\n","out_token_to_int[\"<sos>\"] = sos_token  # Start of sequence token\n","out_token_to_int[\"<eos>\"] = eos_token  # End of sequence token"],"metadata":{"id":"P_yMZrKyA9d_","executionInfo":{"status":"ok","timestamp":1730997212507,"user_tz":-60,"elapsed":12,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def tokenize(a):\n","\n","    x = a.copy()\n","\n","    # Tokenize the Arabizi text (convert each character to its integer representation)\n","    # Convert the Arabizi text to lowercase, then map each character to its corresponding integer from the 'in_token_to_int' dictionary\n","    x.Arabize = [in_token_to_int[i] for i in x.Arabize.lower()]\n","\n","    # Tokenize the Arabic text, adding the start of sequence (sos) token at the beginning and the end of sequence (eos) token at the end\n","    # The start and end tokens are used for sequence modeling\n","    x.Arabic = [sos_token] + [out_token_to_int[i] for i in x.Arabic] + [eos_token]\n","\n","    # Pad the Arabizi sequence to ensure it has a consistent length (in_max)\n","    # If the sequence is shorter than in_max, it is padded with the pad_token (0)\n","    x.Arabize = x.Arabize + (in_max - len(x.Arabize)) * [pad_token]\n","\n","    # Pad the Arabic sequence to ensure it has a consistent length (out_max)\n","    # If the sequence is shorter than out_max, it is padded with the pad_token (0)\n","    x.Arabic = x.Arabic + (out_max - len(x.Arabic)) * [pad_token]\n","\n","    # Return the row with tokenized and padded sequences\n","    return x"],"metadata":{"id":"CDTc5RKkBbaB","executionInfo":{"status":"ok","timestamp":1730997212508,"user_tz":-60,"elapsed":12,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["dataset[[\"Arabize\",\"Arabic\"]] = dataset[[\"Arabize\",\"Arabic\"]].apply(tokenize, axis=1)"],"metadata":{"id":"DFimoShIBYo3","executionInfo":{"status":"ok","timestamp":1730997217688,"user_tz":-60,"elapsed":3380,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2021-06-02T13:53:08.149015Z","iopub.status.busy":"2021-06-02T13:53:08.148689Z","iopub.status.idle":"2021-06-02T13:53:12.724394Z","shell.execute_reply":"2021-06-02T13:53:12.723416Z","shell.execute_reply.started":"2021-06-02T13:53:08.148985Z"},"id":"Q1ipP57M1Ji0","executionInfo":{"status":"ok","timestamp":1730997217700,"user_tz":-60,"elapsed":22,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"outputs":[],"source":["validation = dataset.sample(frac=0.1)\n","train = dataset.drop(validation.index)\n","\n","X_train = train.Arabize\n","y_train = train.Arabic\n","\n","X_valid = validation.Arabize\n","y_valid = validation.Arabic"]},{"cell_type":"markdown","source":["## Model Architecture : :"],"metadata":{"id":"uv0SSxiGB_10"}},{"cell_type":"markdown","source":["#### Positional Encoding :"],"metadata":{"id":"tC6yWtHlC3-n"}},{"cell_type":"markdown","source":["Unlike models like RNNs or LSTMs, which inherently process data sequentially, Transformers process the entire sequence simultaneously. Therefore, positional encoding is used to inject information about the order of tokens in the sequence."],"metadata":{"id":"izDSZ_DfC4Ku"}},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model, dropout=0.1, max_len=9000):\n","        super(PositionalEncoding, self).__init__()\n","\n","        # Dropout layer for regularization during training\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # Learnable scaling parameter for the positional encoding\n","        self.scale = nn.Parameter(torch.ones(1))\n","\n","        # Initialize a tensor for the positional encodings of size (max_len, d_model)\n","        pe = torch.zeros(max_len, d_model)\n","\n","        # Create a tensor of positions from 0 to max_len-1 and reshape to (max_len, 1)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","\n","        # Calculate the scaling factor for each dimension of the positional encoding\n","        # This is based on the formula from the original Transformer paper\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","\n","        # Apply sine function to even indices of the positional encoding\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","\n","        # Apply cosine function to odd indices of the positional encoding\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","\n","        # Reshape the positional encoding tensor to match the shape needed for adding it to the embeddings\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","\n","        # Register the positional encoding tensor as a buffer (not a model parameter)\n","        # This means it will be saved with the model but not updated by backpropagation\n","        self.register_buffer('pe', pe)\n","\n","    # Forward pass: add positional encoding to the input tensor\n","    def forward(self, x):\n","        # Add positional encoding to the input embeddings\n","        # The positional encoding is scaled by the learnable 'scale' parameter\n","        # Only the relevant part of 'pe' is selected based on the sequence length of 'x'\n","        x = x + self.scale * self.pe[:x.size(0), :]\n","\n","        # Apply dropout to the resulting tensor to prevent overfitting\n","        return self.dropout(x)\n"],"metadata":{"id":"yb6ujqhADmYQ","executionInfo":{"status":"ok","timestamp":1730997269250,"user_tz":-60,"elapsed":439,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["#### Transformer model :"],"metadata":{"id":"iZ5Ug-iZEdMA"}},{"cell_type":"code","source":["class TransformerModel(nn.Module):\n","\n","    def __init__(self, intoken, outtoken, hidden, enc_layers=1, dec_layers=1, dropout=0.15, nheads=4):\n","        super(TransformerModel, self).__init__()\n","        # intoken, outtoken : specify the size of the input (Arabizi) and output (Arabic) vocabularies.\n","        # hidden : is the hidden dimension (the number of features in the token embeddings and internal model layers).\n","        # enc_layers, dec_layers : specify the number of encoder and decoder layers.\n","        # dropout : is the dropout rate for regularization.\n","        # nheads : is the number of attention heads used in each attention layer.\n","\n","        # Feed-forward model size, typically 4 times the hidden size\n","        ff_model = hidden * 4\n","\n","        # Encoder: Embedding layer for input tokens (Arabizi)\n","        self.encoder = nn.Embedding(intoken, hidden)\n","        # Positional encoding for the encoder\n","        self.pos_encoder = PositionalEncoding(hidden, dropout)\n","\n","        # Decoder: Embedding layer for output tokens (Arabic)\n","        self.decoder = nn.Embedding(outtoken, hidden)\n","        # Positional encoding for the decoder\n","        self.pos_decoder = PositionalEncoding(hidden, dropout)\n","\n","        # Define encoder layers and create the Transformer encoder\n","        encoder_layers = TransformerEncoderLayer(d_model=hidden, nhead=nheads, dim_feedforward=ff_model, dropout=dropout, activation='relu')\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, enc_layers)\n","\n","        # Define decoder layers and create the Transformer decoder\n","        encoder_layers = TransformerDecoderLayer(hidden, nheads, ff_model, dropout, activation='relu')\n","        self.transformer_decoder = TransformerDecoder(encoder_layers, dec_layers)\n","\n","        # Final linear layer to map the decoder output to the output token space (Arabic tokens)\n","        self.fc_out = nn.Linear(hidden, outtoken)\n","\n","        # Masks for different parts of the model\n","        self.src_mask = None\n","        self.trg_mask = None\n","        self.memory_mask = None\n","\n","\n","    # Generate a mask to prevent attending to future tokens (for autoregressive generation)\n","    def generate_square_subsequent_mask(self, sz, sz1=None):\n","        if sz1 is None:\n","            # Create an upper triangular matrix for masking future tokens\n","            mask = torch.triu(torch.ones(sz, sz), 1)\n","        else:\n","            mask = torch.triu(torch.ones(sz, sz1), 1)\n","        # Mask all future tokens with -inf so that they cannot be attended to\n","        return mask.masked_fill(mask == 1, float('-inf'))\n","\n","\n","    # Create padding mask for the encoder input\n","    def make_len_mask_enc(self, inp):\n","        # Mask padding tokens in the source input (seq_len, batch_size)\n","        return (inp == pad_token).transpose(0, 1)\n","\n","\n","    # Create padding mask for the decoder input\n","    def make_len_mask_dec(self, inp):\n","        # Mask padding tokens in the target input (seq_len, batch_size)\n","        return (inp == pad_token).transpose(0, 1)\n","\n","\n","    # Define the forward pass of the model\n","    def forward(self, src, trg):  # SRC: (seq_len, batch_size)\n","\n","        # Generate target mask (to prevent attending to future tokens during decoding)\n","        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n","            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(trg.device)\n","\n","        # Add padding masks for both source and target\n","        src_pad_mask = self.make_len_mask_enc(src)\n","        trg_pad_mask = self.make_len_mask_dec(trg)\n","\n","        # Encoder: Process source tokens (Arabizi)\n","        src = self.encoder(src)  # (seq_len, batch_size, hidden)\n","        src = self.pos_encoder(src)  # Add positional encoding to source\n","\n","        # Decoder: Process target tokens (Arabic)\n","        trg = self.decoder(trg)  # (seq_len, batch_size, hidden)\n","        trg = self.pos_decoder(trg)  # Add positional encoding to target\n","\n","        # Pass the source through the encoder\n","        memory = self.transformer_encoder(src, None, src_pad_mask)\n","\n","        # Pass the target and memory through the decoder\n","        output = self.transformer_decoder(tgt=trg, memory=memory, tgt_mask=self.trg_mask, memory_mask=None,\n","                                          tgt_key_padding_mask=trg_pad_mask, memory_key_padding_mask=src_pad_mask)\n","\n","        # Output layer: Map the decoder output to the output token space (Arabic)\n","        output = self.fc_out(output)\n","\n","        # Return the model's output\n","        return output\n"],"metadata":{"id":"1tjfrO2vG2yu","executionInfo":{"status":"ok","timestamp":1730997275051,"user_tz":-60,"elapsed":510,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2021-06-02T13:53:12.761117Z","iopub.status.busy":"2021-06-02T13:53:12.760806Z","iopub.status.idle":"2021-06-02T13:53:12.777387Z","shell.execute_reply":"2021-06-02T13:53:12.776267Z","shell.execute_reply.started":"2021-06-02T13:53:12.761085Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"NujjMDHw1Ji6","executionInfo":{"status":"ok","timestamp":1730997276116,"user_tz":-60,"elapsed":13,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}},"outputId":"7bda87ab-35c3-48cf-88de-1524ce7ecb66"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["37"]},"metadata":{},"execution_count":22}],"source":["len(in_token_to_int)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2021-06-02T13:53:12.779577Z","iopub.status.busy":"2021-06-02T13:53:12.779153Z","iopub.status.idle":"2021-06-02T13:53:12.790113Z","shell.execute_reply":"2021-06-02T13:53:12.789263Z","shell.execute_reply.started":"2021-06-02T13:53:12.779537Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"8kkPyGV51Ji7","executionInfo":{"status":"ok","timestamp":1730997276725,"user_tz":-60,"elapsed":16,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}},"outputId":"7e78beb6-c6e4-44fc-9286-462d1dd76327"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["53"]},"metadata":{},"execution_count":23}],"source":["len(out_token_to_int)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZH9bmfPI1Ji8","executionInfo":{"status":"ok","timestamp":1730997277966,"user_tz":-60,"elapsed":332,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}},"outputId":"8f56659b-86aa-4a80-af2f-79f1521294a8"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(\n"]}],"source":["set_seed()\n","model = TransformerModel(len(in_token_to_int), len(out_token_to_int), 128).to(device)"]},{"cell_type":"markdown","source":["#### NoamOpt :"],"metadata":{"id":"wmoKFpWBIsNH"}},{"cell_type":"markdown","source":["The Noam learning rate schedule is designed to prevent the model from training too quickly at the start (which can lead to instability) and to allow the model to \"warm up\" for a smoother training process. <br>\n","The rate increases initially and then decays, ensuring that the model does not get stuck in suboptimal local minima during the later stages of training.<br>\n","This learning rate schedule has been shown to be effective for training large-scale Transformer models like BERT and GPT."],"metadata":{"id":"eSj1YZfBMO5W"}},{"cell_type":"code","source":["class NoamOpt:\n","    \"Optim wrapper that implements rate.\"\n","\n","    def __init__(self, model_size, factor, warmup, optimizer):\n","        # model_size: The dimension of the model (the hidden size of the model's layers (e.g., 512 or 1024) )\n","        # factor: A scaling factor for the learning rate\n","        # warmup: The number of steps during which the learning rate will increase before it starts decaying\n","        # optimizer: The underlying optimizer that will be used to update model parameters\n","\n","        self.optimizer = optimizer  # The optimizer passed to the class (like Adam)\n","        self._step = 0  # Track the number of optimization steps (used for learning rate calculation)\n","        self.warmup = warmup  # Number of warmup steps\n","        self.factor = factor  # Factor used to scale the learning rate\n","        self.model_size = model_size  # The size of the model (usually the hidden dimension)\n","        self._rate = 0  # Store the current learning rate\n","\n","    # Update parameters and learning rate : This method is called during each training step.\n","    def step(self):\n","        \"Update parameters and rate\"\n","        self._step += 1  # Increment step count (each call to this method is a training step)\n","        rate = self.rate()  # Compute the learning rate using the rate function\n","        for p in self.optimizer.param_groups:  # Iterate through the optimizer's parameter groups\n","            p['lr'] = rate  # Set the current learning rate to the computed rate\n","        self._rate = rate  # Update the current learning rate\n","        self.optimizer.step()  # Perform a step of the optimizer to update the model's parameters\n","\n","    # Compute the learning rate at a given step\n","    def rate(self, step = None):\n","        \"Implement `lrate` above\"\n","        if step is None:  # If no step is provided, use the current step\n","            step = self._step\n","\n","        # The Noam learning rate schedule\n","        # The learning rate increases during the warmup phase, then decays as the training progresses\n","        return self.factor * \\\n","            (self.model_size ** (-0.5) *  # Scale by the inverse square root of the model size\n","            min(step ** (-0.5), step * self.warmup ** (-1.5)))  # Apply the warmup and decay formula\n"],"metadata":{"id":"NuLVSrqyKPuQ","executionInfo":{"status":"ok","timestamp":1730997286050,"user_tz":-60,"elapsed":355,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["#### Dataset :"],"metadata":{"id":"21ZOKOqEMYJm"}},{"cell_type":"code","source":["class Arab2ArabizDS(Dataset):\n","    # Custom Dataset class for handling Arabic to Arabizi transliteration tasks\n","\n","    def __init__(self, data, label):\n","        \"\"\"\n","        Initialize the dataset with data and labels.\n","\n","        Parameters:\n","        - data: The source data\n","        - label: The target labels\n","\n","        The data and label are assumed to be pandas DataFrames or Series, and the class converts them to lists for easier handling later.\n","        \"\"\"\n","        self.data = data.values.tolist()  # Convert source data ('Arabizi') into a list of lists\n","        self.labels = label.values.tolist()  # Convert label data ('Arabic') into a list of lists\n","\n","        # Calculate the lengths of the source and label sequences for each item\n","        self.lengths_source = [len(i) for i in data]  # List of lengths for each source item (data)\n","        self.lengths_label = [len(i) for i in label]  # List of lengths for each label item (label)\n","\n","    def __len__(self):\n","        \"\"\"\n","        Return the total number of samples in the dataset.\n","        The length of the dataset is simply the length of the source data.\n","        \"\"\"\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Return a sample from the dataset at the specified index.\n","\n","        Parameters:\n","        - idx: The index of the sample to retrieve from the dataset.\n","\n","        Returns:\n","        - A tuple containing:\n","          - The source data (Arabizi text)\n","          - The target label (Arabic text)\n","          - The length of the source sequence\n","          - The length of the label sequence\n","        \"\"\"\n","        # Return a tuple with the data, label, and their respective lengths\n","        return (self.data[idx], self.labels[idx], self.lengths_source[idx], self.lengths_label[idx])\n"],"metadata":{"id":"FDDsmG36N7SS","executionInfo":{"status":"ok","timestamp":1730997291680,"user_tz":-60,"elapsed":10,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["#### Data Collator :"],"metadata":{"id":"-nA_hOyhPGqA"}},{"cell_type":"markdown","source":["The function data_collator_Arab2Arabiz(data) is a custom data collator designed to process batches of data for training models. The purpose of this collator is to prepare a batch of data by padding the sequences to a consistent length and converting them into tensors suitable for model input."],"metadata":{"id":"AbxgEPTfPz6f"}},{"cell_type":"markdown","source":["Padding Sequences: In natural language processing (NLP) tasks, sequences can vary in length. To feed these sequences into models like transformers, they need to be padded to the same length. This function handles the padding of sequences to the maximum length in the batch. <br>\n","Efficient Batch Processing: Instead of dealing with sequences of different lengths, the collator ensures that all sequences in a batch have the same length, which allows for more efficient batch processing during training.<br>\n","Tensor Conversion: Converts the source and label sequences into PyTorch tensors, which are the format required by neural network models."],"metadata":{"id":"RrJRTAYuRYR3"}},{"cell_type":"code","source":["def data_collator_Arab2Arabiz(data):\n","    # Unzip the list of tuples into separate components: words (source), labels (target), and their respective lengths (source and target sequences).\n","    word, label, length_source, length_label = zip(*data)\n","\n","    # Find the maximum length of the source and label sequences in the batch\n","    tensor_dim_1 = max(length_source)  # Max length of source sequences\n","    tensor_dim_2 = max(length_label)   # Max length of label sequences\n","\n","    # Initialize two tensors filled with pad_token to store the padded sequences\n","    # These tensors will have dimensions:\n","    # - len(word) (number of items in the batch)\n","    # - tensor_dim_1 (maximum length of source sequences)\n","    # - tensor_dim_2 (maximum length of label sequences)\n","\n","    out_word = torch.full((len(word), tensor_dim_1), dtype=torch.long, fill_value=pad_token)\n","    label_word = torch.full((len(word), tensor_dim_2), dtype=torch.long, fill_value=pad_token)\n","\n","    # Iterate through each sequence in the batch and place it into the corresponding tensor\n","    for i in range(len(word)):\n","        # Fill in the padded source sequence (out_word) for each item in the batch\n","        out_word[i][:len(word[i])] = torch.Tensor(word[i])\n","\n","        # Fill in the padded label sequence (label_word) for each item in the batch\n","        label_word[i][:len(label[i])] = torch.Tensor(label[i])\n","\n","    # Return the batch of padded source sequences and label sequences as a tuple\n","    return (out_word, label_word)\n"],"metadata":{"id":"8swda-C9P45l","executionInfo":{"status":"ok","timestamp":1730997298560,"user_tz":-60,"elapsed":8,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["#### K-Sampler :"],"metadata":{"id":"GaCbjjD-QEyN"}},{"cell_type":"markdown","source":["The class KSampler is a custom data sampler used to generate batches of data for training in deep learning models. It inherits from Sampler, which is a PyTorch class used to define how samples are drawn from a dataset. The purpose of this sampler is to shuffle and group data into batches of a specific size, but with an additional focus on maintaining a balanced order based on sequence lengths."],"metadata":{"id":"Mef-U3LCSN3c"}},{"cell_type":"code","source":["class KSampler(Sampler):\n","    # The constructor initializes the sampler.\n","    def __init__(self, data_source, batch_size):\n","        # Extract the lengths of each sample from the data_source (assumed to be a list of tuples).\n","        # The second element in each tuple is the length of the sequence.\n","        self.lens = [x[1] for x in data_source]\n","\n","        # Store the batch size, which defines how many samples should be in each batch.\n","        self.batch_size = batch_size\n","\n","    # The __iter__ method defines how to iterate over the dataset and return batches of data.\n","    def __iter__(self):\n","        # Create a list of indices from 0 to the length of the data source.\n","        idx = list(range(len(self.lens)))\n","\n","        # Pair each index with its corresponding sequence length.\n","        arr = list(zip(self.lens, idx))\n","\n","        # Shuffle the list of (length, index) pairs to randomize the order of the samples.\n","        random.shuffle(arr)\n","\n","        # Define a chunk size (larger than a single batch, used to process data in larger chunks before batching).\n","        n = self.batch_size * 100\n","\n","        # This list will hold the batches of indices.\n","        iterator = []\n","\n","        # Loop through the dataset in steps of size `n` (large chunks).\n","        for i in range(0, len(self.lens), n):\n","            # Take a slice of the data, size `n`.\n","            dt = arr[i:i+n]\n","\n","            # Sort the slice by the sequence lengths to group together sequences of similar lengths.\n","            dt = sorted(dt, key=lambda x: x[0])\n","\n","            # Create smaller batches from the sorted data (batch size is defined by `self.batch_size`).\n","            for j in range(0, len(dt), self.batch_size):\n","                # Extract the indices from the sorted slice and store them in the iterator.\n","                indices = list(map(lambda x: x[1], dt[j:j+self.batch_size]))\n","                iterator.append(indices)\n","\n","        # Shuffle the final batches to ensure randomness in the order.\n","        random.shuffle(iterator)\n","\n","        # Return the flattened list of indices for the batches.\n","        # [item for sublist in iterator for item in sublist] flattens the list of batches.\n","        return iter([item for sublist in iterator for item in sublist])  # Flatten nested list\n","\n","    # The __len__ method returns the total number of items in the dataset.\n","    def __len__(self):\n","        # Return the number of sequences in the dataset (length of `self.lens`).\n","        return len(self.lens)\n"],"metadata":{"id":"9XsGrXZVT4yU","executionInfo":{"status":"ok","timestamp":1730997304668,"user_tz":-60,"elapsed":328,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["Worker-specific Seed: This function is typically used in multi-worker data loading environments (for example, in PyTorch's DataLoader with multiple workers). By ensuring each worker gets a unique but reproducible random seed, it allows parallel data loading with the randomness needed for tasks like shuffling while maintaining deterministic behavior across runs. This is important for reproducibility in experiments."],"metadata":{"id":"n2jqODMmUSZG"}},{"cell_type":"code","execution_count":29,"metadata":{"id":"g1ufdvYU1JjE","executionInfo":{"status":"ok","timestamp":1730997306889,"user_tz":-60,"elapsed":331,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"outputs":[],"source":["def seed_worker(worker_id):\n","    worker_seed = torch.initial_seed() % 2**32\n","    numpy.random.seed(worker_seed)\n","    random.seed(worker_seed)"]},{"cell_type":"markdown","source":["## Training + Validation :"],"metadata":{"id":"aFd-iKSqSVkX"}},{"cell_type":"code","execution_count":30,"metadata":{"id":"yTAT8I9u1JjH","executionInfo":{"status":"ok","timestamp":1730997313395,"user_tz":-60,"elapsed":428,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"outputs":[],"source":["batch_size = 32"]},{"cell_type":"code","source":["train_data = Arab2ArabizDS(X_train, y_train)\n","train_sampler = KSampler(train_data, batch_size)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, worker_init_fn=seed_worker, collate_fn=data_collator_Arab2Arabiz)"],"metadata":{"id":"XA9xHn3YUY5n","executionInfo":{"status":"ok","timestamp":1730997313916,"user_tz":-60,"elapsed":8,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["valid_data = Arab2ArabizDS(X_valid, y_valid)\n","valid_sampler = KSampler(valid_data, batch_size)\n","valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size,worker_init_fn=seed_worker, collate_fn=data_collator_Arab2Arabiz)"],"metadata":{"id":"4gP3SowOUZxZ","executionInfo":{"status":"ok","timestamp":1730997314236,"user_tz":-60,"elapsed":7,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","execution_count":33,"metadata":{"id":"8oMTBwMH1JjI","executionInfo":{"status":"ok","timestamp":1730997320038,"user_tz":-60,"elapsed":5496,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"outputs":[],"source":["criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n","optimizer = NoamOpt(128, 1, 4000 ,optim.Adam(model.parameters(), lr=0))"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"G4sGCpOS1JjI","executionInfo":{"status":"ok","timestamp":1730997419382,"user_tz":-60,"elapsed":539,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"outputs":[],"source":["def run_epoch(iterator):\n","    # Initialize total loss to accumulate the loss over the entire epoch.\n","    total_loss = 0\n","\n","    # Iterate through the data batches provided by the iterator.\n","    for src, trg in iterator:\n","        # Transpose the source (src) and target (trg) sequences to match the model input requirements.\n","        # Move the data to the correct device (CPU or GPU).\n","        src = src.T.to(device)\n","        trg = trg.T.to(device)\n","\n","        # Pass the source and target sequences (excluding the last token of the target) to the model.\n","        output = model(src, trg[:-1, :])\n","\n","        # Reshape the output of the model to match the shape required for computing loss.\n","        # The output shape should be [batch_size * seq_len, vocab_size].\n","        output = output.reshape(-1, output.shape[2])\n","\n","        # Zero out the gradients for the optimizer.\n","        optimizer.optimizer.zero_grad()\n","\n","        # Compute the loss between the model's output and the target sequence (excluding the first token).\n","        # The target is reshaped to be a flat vector.\n","        loss = criterion(output, trg[1:].reshape(-1))\n","\n","        # Accumulate the loss for this batch.\n","        total_loss += loss.item()\n","\n","        # Backpropagate the loss to compute gradients.\n","        loss.backward()\n","\n","        # Clip gradients to avoid exploding gradients (set a maximum threshold for gradient values).\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","\n","        # Update the model's parameters using the optimizer.\n","        optimizer.step()\n","\n","    # Return the average loss over the entire epoch by dividing total loss by the number of batches.\n","    return total_loss / len(iterator)\n"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"syN_5J-O1JjI","executionInfo":{"status":"ok","timestamp":1730997420993,"user_tz":-60,"elapsed":327,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}}},"outputs":[],"source":["def run_validation(iterator):\n","    # Initialize total loss to accumulate the loss over the validation epoch.\n","    total_loss = 0\n","\n","    # Iterate through the validation batches provided by the iterator.\n","    for src, trg in iterator:\n","        # Transpose the source (src) and target (trg) sequences to match the model input requirements.\n","        # Move the data to the correct device (CPU or GPU).\n","        src = src.T.to(device)\n","        trg = trg.T.to(device)\n","\n","        # Pass the source and target sequences (excluding the last token of the target) to the model.\n","        output = model(src, trg[:-1, :])\n","\n","        # Reshape the output of the model to match the shape required for computing loss.\n","        # The output shape should be [batch_size * seq_len, vocab_size].\n","        output = output.reshape(-1, output.shape[2])\n","\n","        # No need to compute gradients during validation, so no zero_grad and no backward pass\n","        # Compute the loss between the model's output and the target sequence (excluding the first token).\n","        # The target is reshaped to be a flat vector.\n","        loss = criterion(output, trg[1:].reshape(-1))\n","\n","        # Accumulate the loss for this batch.\n","        total_loss += loss.item()\n","\n","    # Return the average loss over the entire validation set by dividing total loss by the number of batches.\n","    return total_loss / len(iterator)\n"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5OscliKX1JjL","executionInfo":{"status":"ok","timestamp":1730997934018,"user_tz":-60,"elapsed":511526,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}},"outputId":"a393585d-ad88-4474-cef2-8385dc7cab2a"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH 0 -- 3.124180 -- Val Loss: 2.229038\n","EPOCH 1 -- 1.508315 -- Val Loss: 1.138979\n","EPOCH 2 -- 1.011463 -- Val Loss: 0.926422\n","EPOCH 3 -- 0.893610 -- Val Loss: 0.839303\n","EPOCH 4 -- 0.825758 -- Val Loss: 0.863014\n","EPOCH 5 -- 0.785756 -- Val Loss: 0.775873\n","EPOCH 6 -- 0.752526 -- Val Loss: 0.756501\n","EPOCH 7 -- 0.737339 -- Val Loss: 0.777047\n","EPOCH 8 -- 0.720738 -- Val Loss: 0.740346\n","EPOCH 9 -- 0.688497 -- Val Loss: 0.667755\n","EPOCH 10 -- 0.654240 -- Val Loss: 0.669286\n","EPOCH 11 -- 0.629913 -- Val Loss: 0.620962\n","EPOCH 12 -- 0.603226 -- Val Loss: 0.606131\n","EPOCH 13 -- 0.588001 -- Val Loss: 0.599294\n","EPOCH 14 -- 0.573811 -- Val Loss: 0.601779\n","EPOCH 15 -- 0.552191 -- Val Loss: 0.600437\n","EPOCH 16 -- 0.542038 -- Val Loss: 0.575959\n","EPOCH 17 -- 0.532183 -- Val Loss: 0.540318\n","EPOCH 18 -- 0.526410 -- Val Loss: 0.555283\n","EPOCH 19 -- 0.513015 -- Val Loss: 0.537879\n","EPOCH 20 -- 0.502924 -- Val Loss: 0.547935\n","EPOCH 21 -- 0.492530 -- Val Loss: 0.540649\n","EPOCH 22 -- 0.484088 -- Val Loss: 0.549336\n","EPOCH 23 -- 0.478895 -- Val Loss: 0.520639\n","EPOCH 24 -- 0.474739 -- Val Loss: 0.537886\n","EPOCH 25 -- 0.467944 -- Val Loss: 0.514184\n","EPOCH 26 -- 0.462648 -- Val Loss: 0.519074\n","EPOCH 27 -- 0.452590 -- Val Loss: 0.517818\n","EPOCH 28 -- 0.451636 -- Val Loss: 0.510965\n","EPOCH 29 -- 0.444422 -- Val Loss: 0.497619\n","EPOCH 30 -- 0.434612 -- Val Loss: 0.487985\n","EPOCH 31 -- 0.435273 -- Val Loss: 0.508858\n","EPOCH 32 -- 0.428589 -- Val Loss: 0.499906\n","EPOCH 33 -- 0.428791 -- Val Loss: 0.502524\n","EPOCH 34 -- 0.421433 -- Val Loss: 0.478002\n","EPOCH 35 -- 0.413918 -- Val Loss: 0.499602\n","EPOCH 36 -- 0.415513 -- Val Loss: 0.479414\n","EPOCH 37 -- 0.407195 -- Val Loss: 0.492282\n","EPOCH 38 -- 0.405610 -- Val Loss: 0.485655\n","EPOCH 39 -- 0.406069 -- Val Loss: 0.481806\n","EPOCH 40 -- 0.402601 -- Val Loss: 0.490008\n","EPOCH 41 -- 0.394002 -- Val Loss: 0.470036\n","EPOCH 42 -- 0.392729 -- Val Loss: 0.477485\n","EPOCH 43 -- 0.395171 -- Val Loss: 0.476157\n","EPOCH 44 -- 0.388234 -- Val Loss: 0.459445\n","EPOCH 45 -- 0.383753 -- Val Loss: 0.477440\n","EPOCH 46 -- 0.380457 -- Val Loss: 0.465031\n","EPOCH 47 -- 0.379737 -- Val Loss: 0.466284\n","EPOCH 48 -- 0.373521 -- Val Loss: 0.475689\n","EPOCH 49 -- 0.373804 -- Val Loss: 0.463360\n","EPOCH 50 -- 0.369696 -- Val Loss: 0.464176\n","EPOCH 51 -- 0.367953 -- Val Loss: 0.467895\n","EPOCH 52 -- 0.364698 -- Val Loss: 0.463156\n","EPOCH 53 -- 0.362796 -- Val Loss: 0.458951\n","EPOCH 54 -- 0.359925 -- Val Loss: 0.464667\n","EPOCH 55 -- 0.361345 -- Val Loss: 0.461517\n","EPOCH 56 -- 0.359573 -- Val Loss: 0.454777\n","EPOCH 57 -- 0.357631 -- Val Loss: 0.469036\n","EPOCH 58 -- 0.356664 -- Val Loss: 0.458097\n","EPOCH 59 -- 0.356777 -- Val Loss: 0.459053\n","EPOCH 60 -- 0.348508 -- Val Loss: 0.455695\n","EPOCH 61 -- 0.350910 -- Val Loss: 0.447530\n","EPOCH 62 -- 0.346902 -- Val Loss: 0.453810\n","EPOCH 63 -- 0.347863 -- Val Loss: 0.443474\n","EPOCH 64 -- 0.339616 -- Val Loss: 0.450121\n","EPOCH 65 -- 0.346632 -- Val Loss: 0.450746\n","EPOCH 66 -- 0.341400 -- Val Loss: 0.455603\n","EPOCH 67 -- 0.337201 -- Val Loss: 0.445013\n","EPOCH 68 -- 0.340578 -- Val Loss: 0.447467\n","EPOCH 69 -- 0.335086 -- Val Loss: 0.447107\n","EPOCH 70 -- 0.333929 -- Val Loss: 0.431140\n","EPOCH 71 -- 0.334297 -- Val Loss: 0.439656\n","EPOCH 72 -- 0.331880 -- Val Loss: 0.448099\n","EPOCH 73 -- 0.329667 -- Val Loss: 0.457503\n","EPOCH 74 -- 0.330940 -- Val Loss: 0.448387\n","EPOCH 75 -- 0.329970 -- Val Loss: 0.441077\n","EPOCH 76 -- 0.326197 -- Val Loss: 0.446546\n","EPOCH 77 -- 0.324011 -- Val Loss: 0.437476\n","EPOCH 78 -- 0.322125 -- Val Loss: 0.440159\n","EPOCH 79 -- 0.322598 -- Val Loss: 0.435231\n","EPOCH 80 -- 0.321423 -- Val Loss: 0.436469\n","EPOCH 81 -- 0.323944 -- Val Loss: 0.438357\n","EPOCH 82 -- 0.316319 -- Val Loss: 0.442788\n","EPOCH 83 -- 0.315260 -- Val Loss: 0.434948\n","EPOCH 84 -- 0.313521 -- Val Loss: 0.439292\n","EPOCH 85 -- 0.317522 -- Val Loss: 0.433564\n","EPOCH 86 -- 0.312072 -- Val Loss: 0.457125\n","EPOCH 87 -- 0.313750 -- Val Loss: 0.437518\n","EPOCH 88 -- 0.311782 -- Val Loss: 0.454282\n","EPOCH 89 -- 0.310113 -- Val Loss: 0.434476\n","EPOCH 90 -- 0.309166 -- Val Loss: 0.431031\n","EPOCH 91 -- 0.308494 -- Val Loss: 0.430481\n","EPOCH 92 -- 0.310401 -- Val Loss: 0.438974\n","EPOCH 93 -- 0.307887 -- Val Loss: 0.434938\n","EPOCH 94 -- 0.304375 -- Val Loss: 0.445272\n","EPOCH 95 -- 0.305945 -- Val Loss: 0.433245\n","EPOCH 96 -- 0.303176 -- Val Loss: 0.442861\n","EPOCH 97 -- 0.302245 -- Val Loss: 0.447860\n","EPOCH 98 -- 0.299201 -- Val Loss: 0.440817\n","EPOCH 99 -- 0.302740 -- Val Loss: 0.444355\n"]}],"source":["set_seed()\n","min_loss = 99  # Initialize a variable to track the minimum validation loss seen so far. A high starting value ensures the first validation loss will be smaller.\n","\n","#Change model size\n","for i in range(100):\n","    # Run one epoch of training. This will process the entire training dataset and return the training loss.\n","    loss = run_epoch(train_dataloader)\n","    # Run one epoch of validation. This will process the entire validation dataset and return the validation loss.\n","    loss_val = run_validation(valid_dataloader)\n","\n","    # Check if the current validation loss is lower than the previous minimum validation loss.\n","    if loss_val < min_loss:\n","        min_loss = loss_val # Update the minimum loss to the new lower value.\n","        torch.save(model, \"convert_best\")\n","\n","    print(\"EPOCH %d -- %f -- Val Loss: %f\" % (i, loss, loss_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0tArh7u41JjM"},"outputs":[],"source":["model = torch.load(\"convert_best\").eval()"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gKRrqK9U1JjO","executionInfo":{"status":"ok","timestamp":1730998075838,"user_tz":-60,"elapsed":419,"user":{"displayName":"Louai Azouni","userId":"17463511803402480875"}},"outputId":"b95873b8-e475-47b7-8b54-81796e828e15"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.43048117548789616"]},"metadata":{},"execution_count":37}],"source":["min_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"juEtDzkt1JjP"},"outputs":[],"source":["out_int_to_token = {out_token_to_int[t]:t for t in out_token_to_int}"]},{"cell_type":"markdown","source":["## NOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO"],"metadata":{"id":"abaLtj8pYKMy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ADNm3eK71JjQ"},"outputs":[],"source":["def arabizi_2_arabic(inp):\n","\n","    input_sentence = [in_token_to_int[i] for i in inp.lower()]\n","    preds = [sos_token]\n","\n","    input_sentence = torch.Tensor(input_sentence).unsqueeze(-1).long().to(device)\n","\n","\n","    new_char = -1\n","\n","    while new_char != eos_token:\n","\n","        output_sentence = torch.Tensor(preds).unsqueeze(-1).long().to(device)\n","\n","        src = model.pos_encoder(model.encoder(input_sentence))\n","        trg = model.pos_encoder(model.decoder(output_sentence))\n","\n","        memory = model.transformer_encoder(src)\n","        output = model.transformer_decoder(tgt = trg, memory = memory)\n","\n","        output = model.fc_out(output)\n","        new_char = output.argmax(-1)[-1, 0].item()\n","\n","        preds.append(new_char)\n","\n","        if len(preds) > 50:\n","            break\n","\n","\n","    return \"\".join([out_int_to_token[i] for i in preds[1:-1]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NjfDUgCF1JjV"},"outputs":[],"source":["train = pd.read_csv(\"../input/zindidd/Train.csv\")[[\"textt\", \"labell\"]]\n","train.columns = [\"texts\", \"data_labels\"]\n","\n","data = train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uGSL-wBd1Jjg"},"outputs":[],"source":["def preprocess(text):    #Might use the same setting if they work to other languages (english and french)\n","\n","    text = text.replace('ß',\"b\")\n","    text = text.replace('à',\"a\")\n","    text = text.replace('á',\"a\")\n","    text = text.replace('ç',\"c\")\n","    text = text.replace('è',\"e\")\n","    text = text.replace('é',\"e\")\n","    text = text.replace('$',\"s\")\n","    text = text.replace(\"1\",\"\")\n","\n","\n","    text = text.lower()\n","    text = re.sub(r'[^A-Za-z0-9 ,!?.]', '', text)\n","\n","\n","    # Remove '@name'\n","    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n","\n","    # Replace '&amp;' with '&'\n","    text = re.sub(r'&amp;', '&', text)\n","\n","    # Remove trailing whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    text = re.sub(r'([h][h][h][h])\\1+', r'\\1', text)\n","    text = re.sub(r'([a-g-i-z])\\1+', r'\\1', text)  #Remove repeating characters\n","    text = re.sub(r' [0-9]+ ', \" \", text)\n","    text = re.sub(r'^[0-9]+ ', \"\", text)\n","\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmp3lDVs1Jjj"},"outputs":[],"source":["#Keep numbers block\n","def split(text):\n","\n","    splits = re.findall(r\"[\\w']+|[?!.,]\", text)\n","\n","    to_be_added = []\n","    idx_to_be_added = []\n","\n","    forbidden = [\"?\", \"!\", \".\", \",\"] + known_idx\n","\n","    for i, split in enumerate(splits):\n","\n","        if split in forbidden:\n","            if split in known_idx:\n","                to_be_added.append(known[split])\n","            else:\n","                to_be_added.append(split)\n","            idx_to_be_added.append(i)\n","        #else:\n","        #splits[i] = splits[i][:1000]\n","\n","\n","    splits = [i for i in splits if not i in forbidden]\n","\n","    return splits, to_be_added, idx_to_be_added"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58l_bcOb1Jjr"},"outputs":[],"source":["problematic = []\n","\n","def convert_phrase_2(text):\n","    text = text.replace(\"0\",\"\")\n","    text = text.replace(\"6\",\"\")\n","\n","    #print(\"\\nTEXT: \"+text)\n","    phrase, to_be_added, idx_to_be_added = split(text.lower())\n","\n","    max_len_phrase = max([len(i) for i in phrase])\n","\n","    input_sentence = []\n","    for word in phrase:\n","        input_sentence.append([in_token_to_int[i] for i in word] + [pad_token]*(max_len_phrase-len(word)))\n","\n","    input_sentence = torch.Tensor(input_sentence).long().T.to(device)\n","    preds = [[sos_token] * len(phrase)]\n","\n","    end_word = len(phrase) * [False]\n","    src_pad_mask = model.make_len_mask_enc(input_sentence)\n","\n","\n","    while not all(end_word):\n","        output_sentence = torch.Tensor(preds).long().to(device)\n","\n","        src = model.pos_encoder(model.encoder(input_sentence))\n","        trg = model.pos_encoder(model.decoder(output_sentence))\n","\n","        memory = model.transformer_encoder(src, None ,src_pad_mask)\n","        output = model.transformer_decoder(tgt = trg, memory = memory, memory_key_padding_mask = src_pad_mask)\n","\n","\n","        output = model.fc_out(output)\n","\n","\n","        output = output.argmax(-1)[-1].cpu().detach().numpy()\n","        preds.append(output.tolist())\n","\n","\n","        end_word = (output == eos_token) | end_word\n","\n","        if len(preds) > 50:\n","            global problematic\n","\n","            problematic.append(text)\n","            #print(text)\n","            break\n","\n","\n","    preds = np.array(preds).T\n","    result = []\n","\n","    for word in preds:\n","\n","        tmp = []\n","        for i in word[1:]:\n","            if out_int_to_token[i] == \"<eos>\":\n","                break\n","            tmp.append(out_int_to_token[i])\n","\n","        result.append(\"\".join(tmp))\n","\n","\n","    #Re-add removed punctuation\n","    for item, idx in zip(to_be_added, idx_to_be_added):\n","\n","        if item == \"?\":\n","            item = \"؟\"\n","        elif item == \",\":\n","            item = \"،\"\n","\n","        result.insert(idx, item)\n","\n","\n","    result = \" \".join(result)\n","\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZekQfD8m1Jju"},"outputs":[],"source":["train.texts = train.texts.apply(preprocess)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itGYDAH31Jjw"},"outputs":[],"source":["results = []\n","step_size = 100\n","\n","texts = train.texts.values.tolist()\n","\n","for i in tqdm(range(0, len(texts), step_size)):\n","\n","    out = convert_phrase_2(\" lkrb3 \".join(texts[i:i+step_size]))\n","    splitted_sentences = [ex.lstrip().rstrip() for ex in out.split(\" \" + convert_phrase_2(\"lkrb3\") + \" \")]\n","\n","    if len(splitted_sentences) != len(texts[i:i+step_size]):\n","        print(\"DANGER\")\n","        break\n","\n","    results.extend(splitted_sentences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttPD7gFW1Jjx"},"outputs":[],"source":["train[\"converted\"] = results.copy()\n","train.to_csv(\"train_data.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZkkYAQV71Jjx"},"outputs":[],"source":["test = pd.read_csv(\"../input/zindidd/Test.csv\")\n","test.textt = test.textt.apply(preprocess)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKTma5e01Jlg"},"outputs":[],"source":["results = []\n","step_size = 50\n","\n","texts = test.textt.values.tolist()\n","\n","for i in tqdm(range(0, len(texts), step_size)):\n","\n","    out = convert_phrase_2(\" lkrb3 \".join(texts[i:i+step_size]))\n","    splitted_sentences = [ex.lstrip().rstrip() for ex in out.split(\" \" + convert_phrase_2(\"lkrb3\") + \" \")]\n","\n","    if len(splitted_sentences) != len(texts[i:i+step_size]):\n","        print(\"DANGER\")\n","        break\n","\n","    results.extend(splitted_sentences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQ5pC1WJ1Jlh"},"outputs":[],"source":["test[\"converted\"] = results\n","test.to_csv(\"test_data.csv\")"]},{"cell_type":"markdown","metadata":{"id":"R6y0gw241Jli"},"source":["--------------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nlEdYAZd1Jlq"},"outputs":[],"source":["def preprocessing_for_bert(data, tokenizer, preprocess_text, max_len=256):\n","\n","    input_ids = []\n","    attention_masks = []\n","    tmp = tokenizer.encode(\"ab\")[-1]\n","\n","    for sentence in data:\n","\n","        encoding = tokenizer.encode(preprocess_text(sentence))\n","\n","        if len(encoding) > max_len:\n","            encoding = encoding[:max_len-1] + [tmp]\n","\n","        in_ids = encoding\n","        att_mask = [1]*len(encoding)\n","\n","        input_ids.append(in_ids)\n","        attention_masks.append(att_mask)\n","\n","    return input_ids, attention_masks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K3Lghg2O1Jls"},"outputs":[],"source":["class BertDataset(Dataset):\n","\n","    def __init__(self, data, masks, label=None):\n","\n","        self.data = data\n","        self.masks = masks\n","\n","        if label != None:\n","            self.labels = label\n","        else:\n","            self.labels = None\n","\n","        self.lengths = [len(i) for i in data]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        if self.labels !=  None:\n","            return (self.data[idx], self.masks[idx], self.labels[idx], self.lengths[idx])\n","        else:  #For validation\n","            return (self.data[idx], self.masks[idx], None, self.lengths[idx])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1ZJYVw01Jlu"},"outputs":[],"source":["def data_collator(data):\n","\n","    sentence, mask, label, length = zip(*data)\n","\n","    tensor_dim = max(length)\n","\n","    out_sentence = torch.full((len(sentence), tensor_dim), dtype=torch.long, fill_value=pad)\n","    out_mask = torch.zeros(len(sentence), tensor_dim, dtype=torch.long)\n","\n","    for i in range(len(sentence)):\n","\n","        out_sentence[i][:len(sentence[i])] = torch.Tensor(sentence[i])\n","        out_mask[i][:len(mask[i])] = torch.Tensor(mask[i])\n","\n","    if label[0] != None:\n","        return (out_sentence, out_mask, torch.Tensor(label).long())\n","    else:\n","        return (out_sentence, out_mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lB9tdYGJ1Jlw"},"outputs":[],"source":["class KSampler(Sampler):\n","\n","    def __init__(self, data_source, batch_size):\n","        self.lens = [x[1] for x in data_source]\n","        self.batch_size = batch_size\n","\n","    def __iter__(self):\n","\n","        idx = list(range(len(self.lens)))\n","        arr = list(zip(self.lens, idx))\n","\n","        random.shuffle(arr)\n","        n = self.batch_size*100\n","\n","        iterator = []\n","\n","        for i in range(0, len(self.lens), n):\n","            dt = arr[i:i+n]\n","            dt = sorted(dt, key=lambda x: x[0])\n","\n","            for j in range(0, len(dt), self.batch_size):\n","                indices = list(map(lambda x: x[1], dt[j:j+self.batch_size]))\n","                iterator.append(indices)\n","\n","        random.shuffle(iterator)\n","        return iter([item for sublist in iterator for item in sublist])  #Flatten nested list\n","\n","    def __len__(self):\n","        return len(self.lens)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VFmt9qZU1Jly"},"outputs":[],"source":["# Create the BertClassfier class\n","class BertClassifier(nn.Module):\n","\n","    def __init__(self, model_name, dropout, freeze_bert=False):\n","\n","        super(BertClassifier, self).__init__()\n","        D_in, H, D_out = 768, 200, 3\n","\n","        self.bert = AutoModel.from_pretrained(model_name)\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(D_in, H),\n","            nn.ReLU(),\n","            nn.Linear(H, D_out)\n","        )\n","\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, input_ids, attention_mask):\n","\n","        outputs = self.bert(input_ids=input_ids,\n","                            attention_mask=attention_mask)\n","\n","        last_hidden_state_cls = outputs[0][:, 0, :]\n","\n","        logits = self.classifier(last_hidden_state_cls)\n","\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAPLDhCz1Jlz"},"outputs":[],"source":["def initialize_model(model_name, epochs=4, dropout=0.1):\n","\n","    bert_classifier = BertClassifier(model_name, dropout=dropout, freeze_bert=False)\n","\n","    bert_classifier.to(device)\n","\n","    optimizer = AdamW(bert_classifier.parameters(),\n","                      lr=5e-5,\n","                      eps=1e-8\n","                      )\n","\n","    total_steps = len(train_dataloader) * epochs\n","\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps=0, # Default value\n","                                                num_training_steps=total_steps)\n","    return bert_classifier, optimizer, scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qM-HG-t31Jl0"},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","\n","def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False, fold=0, prefix=\"\"):\n","\n","    global max_acc\n","\n","    print(\"Start training...\\n\")\n","    for epoch_i in range(epochs):\n","\n","        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n","        print(\"-\"*70)\n","\n","        t0_epoch, t0_batch = time.time(), time.time()\n","\n","        total_loss, batch_loss, batch_counts = 0, 0, 0\n","        model.train()\n","\n","        for step, batch in enumerate(train_dataloader):\n","            batch_counts +=1\n","\n","            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","            model.zero_grad()\n","\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","            loss = loss_fn(logits, b_labels)\n","            batch_loss += loss.item()\n","            total_loss += loss.item()\n","\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            optimizer.step()\n","            scheduler.step()\n","\n","            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n","\n","                time_elapsed = time.time() - t0_batch\n","\n","                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n","\n","                batch_loss, batch_counts = 0, 0\n","                t0_batch = time.time()\n","\n","            if step%200 == 0 and step != 0 and epoch_i != 0 and epoch_i != 1:\n","\n","                print(\"-\"*70)\n","\n","                if evaluation == True:\n","\n","                    val_loss, val_accuracy = evaluate(model, val_dataloader)\n","\n","                    if val_accuracy > max_acc:\n","                        max_acc = val_accuracy\n","                        torch.save(model, prefix + \"_best_\"+str(fold))\n","                        print(\"new max\")\n","\n","\n","                    print(val_accuracy)\n","\n","                    print(\"-\"*70)\n","                print(\"\\n\")\n","\n","                model.train()\n","\n","        avg_train_loss = total_loss / len(train_dataloader)\n","\n","        print(\"-\"*70)\n","\n","        if evaluation == True:\n","\n","            val_loss, val_accuracy = evaluate(model, val_dataloader)\n","\n","            if val_accuracy > max_acc:\n","                max_acc = val_accuracy\n","                torch.save(model, prefix+\"_best_\"+str(fold))\n","                print(\"new max\")\n","\n","            time_elapsed = time.time() - t0_epoch\n","\n","            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n","            print(\"-\"*70)\n","        print(\"\\n\")\n","\n","    print(\"Training complete!\")\n","\n","\n","def evaluate(model, val_dataloader):\n","\n","    model.eval()\n","\n","    val_accuracy = []\n","    val_loss = []\n","\n","    for batch in val_dataloader:\n","\n","        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","        loss = loss_fn(logits, b_labels)\n","        val_loss.append(loss.item())\n","\n","        preds = torch.argmax(logits, dim=1).flatten()\n","\n","        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n","        val_accuracy.append(accuracy)\n","\n","    val_loss = np.mean(val_loss)\n","    val_accuracy = np.mean(val_accuracy)\n","\n","    return val_loss, val_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avnwMHi21Jl1"},"outputs":[],"source":["def get_indices(arr, idxs):  #Helper function to get multiple indexes from a list\n","\n","    output = []\n","    for idx in idxs:\n","        output.append(arr[idx])\n","\n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fbMHuMcM1Jl2"},"outputs":[],"source":["#Tried these different preprocessing functions and tesed their effect on the results\n","#Found out that text_preprocessing_2 gives the best results for the English model\n","def text_preprocessing_1(text):\n","\n","    text = text.lower()\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    return text\n","\n","\n","def text_preprocessing_2(text):\n","\n","    text = text.lower()\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    text = re.sub(r'([a-g-i-z][a-g-i-z])\\1+', r'\\1', text)\n","\n","    return text\n","\n","\n","def text_preprocessing_3(text):\n","\n","    text = text.replace('ß',\"b\")\n","    text = text.replace('à',\"a\")\n","    text = text.replace('á',\"a\")\n","    text = text.replace('ç',\"c\")\n","    text = text.replace('è',\"e\")\n","    text = text.replace('é',\"e\")\n","    text = text.replace('$',\"s\")\n","    text = text.replace(\"1\",\"\")\n","\n","\n","    text = text.lower()\n","    text = re.sub(r'[^A-Za-z0-9 ,!?.]', '', text)\n","\n","\n","    # Remove '@name'\n","    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n","\n","    # Replace '&amp;' with '&'\n","    text = re.sub(r'&amp;', '&', text)\n","\n","    # Remove trailing whitespace\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    text = re.sub(r'([h][h][h][h])\\1+', r'\\1', text)\n","    text = re.sub(r'([a-g-i-z])\\1+', r'\\1', text)  #Remove repeating characters\n","    text = re.sub(r' [0-9]+ ', \" \", text)\n","    text = re.sub(r'^[0-9]+ ', \"\", text)\n","\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E9g9BXOa1Jl3"},"outputs":[],"source":["data = pd.read_csv(\"../input/zindidd/Train.csv\")[[\"textt\", \"labell\"]].iloc[1000:]\n","data.columns = [\"texts\", \"data_labels\"]\n","\n","data.data_labels = data.data_labels.replace(0,2)  #Neutral 2, Positive 1, Negative 0\n","data.data_labels = data.data_labels.replace(-1,0)\n","\n","\n","\n","X = data.texts.values\n","y = data.data_labels.values\n","\n","preprocessed_data, masks = preprocessing_for_bert(X, tokenizer_en, text_preprocessing_2, max_len=256)\n","pad = tokenizer_en.pad_token_id"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOqEBCPg1Jl6"},"outputs":[],"source":["kfold = KFold(5, True, seed)\n","fold = 0\n","\n","bests = []\n","\n","for train_ids, val_ids in kfold.split(preprocessed_data):\n","\n","    print(\"\\n\\tFOLD %d \\n\" % (fold))\n","    max_acc = -99\n","\n","    X_train = get_indices(preprocessed_data, train_ids)\n","    y_train = get_indices(y, train_ids)\n","    train_masks = get_indices(masks, train_ids)\n","\n","    X_val = get_indices(preprocessed_data, val_ids)\n","    y_val = get_indices(y, val_ids)\n","    val_masks = get_indices(masks, val_ids)\n","\n","\n","    X_val, y_val, val_masks = list(zip(*sorted(zip(X_val, y_val, val_masks), key=lambda x: len(x[0]))))  #Order the validation data for faster validation\n","    X_val, y_val, val_masks = list(X_val), list(y_val), list(val_masks)\n","\n","\n","    # Convert other data types to torch.Tensor\n","    y_train = torch.tensor(y_train)\n","    y_val = torch.tensor(y_val)\n","\n","    # Create the DataLoader for our training set\n","    train_data = BertDataset(X_train, train_masks, y_train)\n","    train_sampler = KSampler(train_data, batch_size)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, collate_fn=data_collator)\n","\n","    # Create the DataLoader for our validation set\n","    val_data = BertDataset(X_val, val_masks, y_val)\n","    val_sampler = SequentialSampler(val_data)\n","    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size, collate_fn=data_collator)\n","\n","\n","    set_seed()    # Set seed for reproducibility\n","    bert_classifier, optimizer, scheduler = initialize_model(model_name=model_name_en, epochs=n_epochs, dropout=0.05)\n","    train(bert_classifier, train_dataloader, val_dataloader, epochs=n_epochs, evaluation=True, fold=fold, prefix=\"en\")\n","\n","    fold += 1\n","    bests.append(max_acc)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UN71ZM1e1Jl9"},"outputs":[],"source":["bests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r-tvVXnd1Jl-"},"outputs":[],"source":["data = pd.read_csv(\"train_data.csv\")[[\"converted\", \"data_labels\"]].iloc[1000:]\n","data.columns = [\"texts\", \"data_labels\"]\n","\n","data.data_labels = data.data_labels.replace(0,2)  #Neutral 2, Positive 1, Negative 0\n","data.data_labels = data.data_labels.replace(-1,0)\n","\n","\n","\n","X = data.texts.values\n","y = data.data_labels.values\n","\n","preprocessed_data, masks = preprocessing_for_bert(X, tokenizer_ar, lambda x: x, max_len=256)\n","pad = tokenizer_ar.pad_token_id"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gxaJVkK71Jl_"},"outputs":[],"source":["kfold = KFold(10, True, seed)\n","fold = 0\n","\n","bests = []\n","\n","for train_ids, val_ids in kfold.split(preprocessed_data):\n","\n","    print(\"\\n\\tFOLD %d \\n\" % (fold))\n","    max_acc = -99\n","\n","    X_train = get_indices(preprocessed_data, train_ids)\n","    y_train = get_indices(y, train_ids)\n","    train_masks = get_indices(masks, train_ids)\n","\n","    X_val = get_indices(preprocessed_data, val_ids)\n","    y_val = get_indices(y, val_ids)\n","    val_masks = get_indices(masks, val_ids)\n","\n","\n","    X_val, y_val, val_masks = list(zip(*sorted(zip(X_val, y_val, val_masks), key=lambda x: len(x[0]))))  #Order the validation data for faster validation\n","    X_val, y_val, val_masks = list(X_val), list(y_val), list(val_masks)\n","\n","\n","    # Convert other data types to torch.Tensor\n","    y_train = torch.tensor(y_train)\n","    y_val = torch.tensor(y_val)\n","\n","    # Create the DataLoader for our training set\n","    train_data = BertDataset(X_train, train_masks, y_train)\n","    train_sampler = KSampler(train_data, batch_size)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, collate_fn=data_collator)\n","\n","    # Create the DataLoader for our validation set\n","    val_data = BertDataset(X_val, val_masks, y_val)\n","    val_sampler = SequentialSampler(val_data)\n","    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size, collate_fn=data_collator)\n","\n","\n","    set_seed()    # Set seed for reproducibility\n","    bert_classifier, optimizer, scheduler = initialize_model(model_name=model_name_ar, epochs=n_epochs, dropout=0)\n","    train(bert_classifier, train_dataloader, val_dataloader, epochs=n_epochs, evaluation=True, fold=fold, prefix=\"ar\")\n","\n","    fold += 1\n","    bests.append(max_acc)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KCdwiiTz1Jl_"},"outputs":[],"source":["bests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"etsnYPkU1JmA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eFh67ZrL1JmB"},"outputs":[],"source":["def bert_single_predict(model, test_dataloader):\n","\n","    model.eval()\n","\n","    all_logits = []\n","\n","    for batch in tqdm(test_dataloader):\n","\n","        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n","\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","        all_logits.append(logits)\n","\n","    all_logits = torch.cat(all_logits, dim=0)\n","\n","    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n","\n","    return probs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDO0wRFw1JmC"},"outputs":[],"source":["def bert_ensemble_predict(sentences, models, tokenizer, preprocess, truncate=True, max_len=256):\n","\n","    inputs, masks = preprocessing_for_bert(sentences, tokenizer, preprocess, max_len=max_len)\n","\n","\n","    dataset = BertDataset(inputs, masks)\n","    sample = SequentialSampler(dataset)\n","    dataloader = DataLoader(dataset, sampler=sample, batch_size=128, collate_fn=data_collator)\n","\n","    preds = []\n","\n","    for model in models:\n","        preds.append(bert_single_predict(model, dataloader))\n","\n","    return preds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKdNECSW1JmC"},"outputs":[],"source":["def predict_lang(lang_prefix, directory, preprocess_fn, dataset, model_name, n=1, truncate=True, max_len=256):\n","\n","    print(\"Loading the models ....\")\n","\n","    global pad\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","    pad = tokenizer.pad_token_id\n","\n","    lang_models = []\n","    for i in range(n):\n","        lang_models.append(torch.load(directory + \"/\" + lang_prefix + \"best_\"+str(i), map_location=device))\n","\n","    print(\"Inference ....\")\n","\n","    out = bert_ensemble_predict(dataset, lang_models, tokenizer, preprocess_fn, truncate=truncate, max_len=max_len)\n","\n","    out_sum = out[0]\n","    for i in range(1,n):\n","        out_sum = out[i] + out_sum\n","\n","    return out_sum"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kdyWJvxA1JmH"},"outputs":[],"source":["#Sort the list for faster inference\n","df = pd.read_csv(\"../input/zindidd/Test.csv\")\n","df_converted = pd.read_csv(\"test_data.csv\")\n","\n","df[\"lens\"] = df.textt.apply(len)\n","df = df.sort_values(by=\"lens\").set_index(\"IDD\", drop=True)\n","df_converted = df_converted.set_index(\"IDD\", drop=True).loc[df.index]\n","\n","\n","#Convert to list\n","test = df.textt.tolist()\n","test_converted = df_converted[[\"converted\"]].converted.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sx1pNrBP1JmH"},"outputs":[],"source":["output_ar = predict_lang(\"ar_\", \"./\", lambda x:x, test_converted, model_name_ar, n=10, truncate=True, max_len=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44PqCqYG1JmI"},"outputs":[],"source":["output_en = predict_lang(\"en_\", \"./\", text_preprocessing_2, test, model_name_en, n=5, truncate=True, max_len=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rx91s7051JmI"},"outputs":[],"source":["df[\"preds\"] = ((output_ar/10)*1.30+(output_en/5)).argmax(1)\n","\n","df.preds = df.preds.replace(0,-1)\n","df.preds = df.preds.replace(2,0)\n","\n","the_output = df.reset_index()[[\"IDD\", \"preds\"]]\n","the_output.columns = [\"ID\", \"label\"]\n","\n","the_output.to_csv(\"lessvalid_convvalid150.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nAvHLB3y1JmJ"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}